# GPT-OSS CLI Chat Environment Configuration
# Copy this file to .env and customize as needed

# ============================================================================
# Model Configuration
# ============================================================================

# Model variant to load
# Options: gpt-j-20b, gpt-neox-20b
MODEL_NAME=gpt-j-20b

# Optional: Override HuggingFace model path
# Leave empty to use default path for MODEL_NAME
MODEL_PATH=/models

# Quantization level for memory optimization
# Options: int4, int8, fp16, none
# Recommended: int4 for best memory/quality tradeoff on M1/M2 Macs
QUANTIZATION=int4

# Compute device selection
# Options: auto, mps, cuda, cpu
# Note: auto will select mps on Apple Silicon
DEVICE=auto

# Number of GPUs for tensor parallelism (multi-GPU support)
TENSOR_PARALLEL_SIZE=1

# GPU memory utilization (0.5-1.0)
# Recommended: 0.90 for dedicated inference, 0.70 if running other apps
GPU_MEMORY_UTIL=0.7

# Maximum context window size in tokens (512-32768)
# Recommended: 4096 for GPT-J/NeoX, adjust based on available memory
MAX_MODEL_LEN=32768

# Defer model loading until first use
LAZY_LOAD=true

# Run warmup generation after loading (recommended for accurate latency)
WARMUP=true

# Allow loading custom model code from HuggingFace
TRUST_REMOTE_CODE=true

# ============================================================================
# Inference Configuration
# ============================================================================

# Sampling temperature (0.0-2.0)
# Lower = more focused, Higher = more random
# Recommended: 0.7 for chat, 0.3 for code generation
TEMPERATURE=0.7

# Nucleus sampling threshold (0.0-1.0)
# Consider only tokens with cumulative probability <= top_p
TOP_P=0.9

# Top-k sampling limit (-1 to disable)
# Consider only the top k highest probability tokens
TOP_K=50

# Repetition penalty (1.0 = no penalty, higher = more penalty)
# Recommended: 1.0-1.2 for natural conversation
REPETITION_PENALTY=1.0

# Maximum tokens to generate (1-4096)
# Note: This is per response, not total conversation length
MAX_TOKENS=4096

# Enable token-by-token streaming output
STREAMING=true

# Stop sequences (comma-separated)
# Generation stops when any of these sequences are produced
# Example: STOP_SEQUENCES=\n\nUser:,\n\nAssistant:,<|endoftext|>
STOP_SEQUENCES=

# ============================================================================
# CLI Configuration
# ============================================================================

# Enable colored terminal output
COLORIZE=true

# Display token counts for inputs and outputs
SHOW_TOKENS=true

# Display generation time and tokens/second
SHOW_LATENCY=true

# Enable debug logging
VERBOSE=false

# Automatically save conversation history on exit
AUTO_SAVE=false

# Path to conversation history file
HISTORY_FILE=.chat_history.json

# Enable command autocompletion
ENABLE_AUTOCOMPLETE=true

# Enable multi-line message input
# When true, press Alt+Enter to submit (Enter adds new line)
# When false, press Enter to submit
MULTILINE_INPUT=true

# ============================================================================
# Advanced Settings
# ============================================================================

# Python logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# MLX device selection override (usually auto-detected)
# MLX_DEVICE=gpu

# ============================================================================
# Development Settings (for contributors)
# ============================================================================

# Enable development mode features
# DEV_MODE=false

# Profile model loading and inference
# ENABLE_PROFILING=false

# Cache directory for downloaded models
# CACHE_DIR=~/.cache/huggingface
